{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fb39bfef-8618-4a41-a07d-083143bfd361",
    "_uuid": "a3ee560a-142c-45f2-9ffb-5a098dc753b0"
   },
   "source": [
    "# FCNN (Fully Convolutional Neural Network)\n",
    "\n",
    "Este tipo de arquitecturas son conformadas por dos bloques principales, que conforman una trayectoria de codificador-decodificador o una trayectoria expansiva de contracción equivalente. En la figura se observan estas dos etapas y se muestra un ejemplo de cómo las características extraídas se hacen más particulares o especificas a medida que se profundiza en la red.\n",
    "\n",
    "Los codificadores-decodificadores pueden definirse así:\n",
    "\n",
    "* **Codificador (parte izquierda de la red)**: Codifica la imagen en una representación abstracta de las características de la imagen aplicando una secuencia de bloques convolucionales que disminuyen gradualmente la altura y la anchura de la representación, pero un número creciente de canales que corresponden a las características de la imagen.\n",
    "\n",
    "* **Decodificador (parte derecha de la red)**: Decodifica la representación de la imagen en una máscara binaria aplicando una secuencia de convoluciones ascendentes (NO es lo mismo que la deconvolución) que aumenta gradualmente la altura y la anchura de la representación hasta el tamaño de la imagen original y disminuye el número de canales hasta el número de clases que estamos segmentando.\n",
    "\n",
    "    \n",
    "<div style=\"width:100%;text-align: center;\">\n",
    "<img src=\"https://i.imgur.com/WNCNVHS.png\" width=\"800\" height=\"400\"/>\n",
    "</div>\n",
    "\n",
    "La FCNN es la arquitectura más utilizada para la segmentación de imágenes, entre ellas, se utiliza una en particular para imagenes médicas y se conoce como red U-net la cual tiene conexiones entre los dos bloques principales con el fin de aportar información a la etapa de reconstrucción del mapa de características."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6777468b-899d-4c76-ada3-6cd225466cc7",
    "_uuid": "64d4040c-0c8d-4bc5-b64e-554c06052de3"
   },
   "source": [
    "# Objetivo de las redes U-Net\n",
    "U-Net es una arquitectura de red convolucional para la segmentación rápida y precisa de imágenes. La intención de U-Net es captar tanto las características del contexto como las de la ubicación. La idea principal de la aplicación es utilizar capas de contracción sucesivas, a las que siguen inmediatamente operadores de remuestreo para obtener salidas de mayor resolución en las imágenes de entrada. \n",
    "# Arquitectura UNET\n",
    "  \n",
    "\n",
    "<div>\n",
    "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=\"900\" height=\"800\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Al visualizar la forma de la arquitectura de la red, podemos ver por qué probablemente se llama U-Net. La arquitectura tiene forma de U, de ahí su nombre. La arquitectura de la red se ilustra en la figura anterior. Consta de una vía de contracción (lado izquierdo) y una vía de expansión (lado derecho) al igual que las FCNN y además contiene la conexión entre los dos bloques como se mencionó anteriormente. La ruta de contracción sigue la arquitectura típica de una red convolucional (CNN)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7670eb86-c6b6-46a4-b338-685caaf001c9",
    "_uuid": "dab1132a-8486-4526-b3c8-dbdb84a9bc3d"
   },
   "source": [
    "# Database para aplicar modelo U-net\n",
    "La base de datos utilizada consiste en 800 imágenes de radiografía de tórax donde 394 imágenes corresponden a una manifestación de tuberculosis y 406 corresponden a radigrafías normales o de personas sanas. Esta base de datos es una combinación de dos bases de datos conocidas en el estado del arte para radiografía de tórax (Montgomery y Shenzen database).\n",
    "\n",
    "## Etapas de pre-procesamiento y entrenamiento\n",
    "1. Cargar la base de datos\n",
    "2. Preparación de los datos\n",
    "3. Imágenes de entrenamiento y de prueba\n",
    "4. Construyendo una red U-net\n",
    "5. Entrenamiento del modelo U-net\n",
    "6. Métricas y validacón"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0dafc0a7-c0a6-435f-be7e-0476474a0fd8",
    "_uuid": "2d7e0872-6e6e-491b-b507-6de922fbbacc"
   },
   "source": [
    "# Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-gpu 2.11.0 requires keras<2.12,>=2.11.0, but you have keras 2.15.0 which is incompatible.\n",
      "tensorflow-gpu 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.5 which is incompatible.\n",
      "tensorflow-gpu 2.11.0 requires tensorboard<2.12,>=2.11, but you have tensorboard 2.15.2 which is incompatible.\n",
      "tensorflow-gpu 2.11.0 requires tensorflow-estimator<2.12,>=2.11.0, but you have tensorflow-estimator 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python pandas numpy tensorflow==2.15.1 seaborn tqdm scikit-image pennylane pennylane-lightning-gpu scikit-learn custatevec_cu12 torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.15\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "5c48926a-69ac-4877-af32-9fd330f6ea8a",
    "_uuid": "c00aafff-b078-4bf6-aaf1-d1a27996554e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image # Librería de edición de imágenes\n",
    "from tqdm import tqdm # librería para monitorear las funciones y sus tiempos de ejecución.\n",
    "import cv2 # Librería Open-CV para python.\n",
    "\n",
    "from skimage import segmentation\n",
    "from glob import glob\n",
    "from collections import defaultdict # Define un diccionario\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set(font_scale = 2)\n",
    "\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 15:02:44.842619: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-29 15:02:44.891387: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-29 15:02:44.891415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-29 15:02:44.892735: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-29 15:02:44.900608: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-29 15:02:45.820415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 15:02:46.783348: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-29 15:02:46.840895: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.1\n",
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ead84576-d40c-4322-af33-11dcb6fac46d",
    "_uuid": "12a083b4-e404-45a7-a34b-00505fda9f1c"
   },
   "source": [
    "## 1. Cargar la base de datos \n",
    "\n",
    "Sobre los datos:\n",
    "* Hay una pequeña anormalidad en la convención de nombres de las máscaras.\n",
    "* Algunas imágenes no tienen sus correspondientes máscaras.\n",
    "\n",
    "Comprobación de las radiografías y sus respectivas máscaras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip data.zip -d chest-xray-data/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "97cb7e8e-a383-4f1d-a717-aed2d95b957f",
    "_uuid": "fe58016f-8476-4635-a1e0-3a51f9cdb7bd",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Declarando los directorios de las imágenes\n",
    "DIR = \"./chest-xray-data/data/\"\n",
    "\n",
    "lung_image_paths = glob(os.path.join(DIR,\"Lung Segmentation/CXR_png/*.png\")) # Directorio de las imagenes radiográficas.\n",
    "mask_image_paths = glob(os.path.join(DIR,\"Lung Segmentation/masks/*.png\")) # Directorio de las máscaras, que indican la localización de pulmones.\n",
    "\n",
    "related_paths = defaultdict(list)\n",
    "\n",
    "# Combinando las imágenes de 1 para 1\n",
    "for img_path in lung_image_paths:\n",
    "    img_match = re.search(\"CXR_png/(.*)\\\\.png$\", img_path)\n",
    "    if img_match:\n",
    "        img_name = img_match.group(1)\n",
    "    for mask_path in mask_image_paths:\n",
    "        mask_match = re.search(img_name, mask_path)\n",
    "        if mask_match:\n",
    "            related_paths[\"image_path\"].append(img_path)\n",
    "            related_paths[\"mask_path\"].append(mask_path)\n",
    "\n",
    "paths_df = pd.DataFrame.from_dict(related_paths)\n",
    "paths_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "adf8bddf-1372-4fd3-b625-791f81e24666",
    "_uuid": "cd373fb9-395d-4d67-8ebe-dced71a2b533",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Nimgs = paths_df.shape[0] # Número total de imagenes\n",
    "print(\"Número total de imagenes\", Nimgs)\n",
    "@interact(xray_num = (0,Nimgs-1,1))\n",
    "def plot_pair_img_mask(xray_num):\n",
    "    img_path = paths_df[\"image_path\"][xray_num]\n",
    "    mask_path = paths_df[\"mask_path\"][xray_num]\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "    mask = Image.open(mask_path)\n",
    "    \n",
    " ## Visualizar imagen e independientemente su máscara\n",
    "    fig = plt.figure(figsize = (10,10))\n",
    "\n",
    "    ax1 = fig.add_subplot(2,2,1)\n",
    "    ax1.imshow(img, cmap = \"gray\")\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    ax2 = fig.add_subplot(2,2,2)\n",
    "    plt.axis('off')\n",
    "    ax2.imshow(mask, cmap = \"viridis\")\n",
    "    plt.title(\"Máscara real\")\n",
    "    fig.show()\n",
    "\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(img, cmap = \"gray\")\n",
    "    edges_est = segmentation.clear_border(np.squeeze(mask))\n",
    "    plt.contour(edges_est,[0.5],colors=['red'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "353cad3c-cb50-4d1e-b2c2-65aad93cad62",
    "_uuid": "976f5a19-97da-44f7-9da1-33e1680b2fb3"
   },
   "source": [
    "## 2. Preparación de los datos  \n",
    "En este paso crearemos una función para tratar las imágenes de rayos X y las máscaras. Este proceso es necesario para poder preprocesar y normalizar las imágenes.  \n",
    "Utilizaremos la biblioteca `cv2` para redimensionar las imágenes y las máscaras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f9773670-b53f-446f-86fa-1b7ebb52e9f5",
    "_uuid": "8dfeffea-b25e-481d-9386-428f0abda1d2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "img_side_size = 256 # tamaño de las imágenes a procesar.\n",
    "\n",
    "def prepare_train_test(df = pd.DataFrame(), resize_shape = tuple(), color_mode = \"gray\"):\n",
    "    img_array = list()\n",
    "    mask_array = list()\n",
    "\n",
    "    # Preparando Imagenes\n",
    "    for image_path in tqdm(paths_df.image_path):\n",
    "        resized_image = cv2.resize(cv2.imread(image_path),resize_shape) # Reescalando las imagenes a una resolución dada por el parámetro resize_shape\n",
    "        resized_image = resized_image/255. # Normalizamos las intensidades de la imágen a valores entre 0 y 1. \n",
    "        # Procesar según el tipo de imagen (Color --> RGB | escala de grises --> Gray)\n",
    "        if color_mode == \"gray\":\n",
    "            img_array.append(resized_image[:,:,0])\n",
    "        elif color_mode == \"rgb\":\n",
    "            img_array.append(resized_image[:,:,:])\n",
    "            \n",
    "    # Preparando las mascaras\n",
    "    for mask_path in tqdm(paths_df.mask_path):\n",
    "        resized_mask = cv2.resize(cv2.imread(mask_path),resize_shape)\n",
    "        resized_mask = resized_mask/255.\n",
    "        mask_array.append(resized_mask[:,:,0])\n",
    "\n",
    "    return img_array, mask_array\n",
    "\n",
    "img_array, mask_array = prepare_train_test(df = paths_df, resize_shape = (img_side_size,img_side_size), color_mode = \"gray\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1ce168ee-4d5e-4897-8901-1340a563c59c",
    "_uuid": "16f7556f-802a-49b9-be6a-4fdb263d0f79"
   },
   "source": [
    "## 3. Imágenes de entrenamiento y de prueba\n",
    "Separación de los datos de entrenamiento y de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7dce0c4e-a684-4dc8-9b01-038a664ff53c",
    "_uuid": "0d74326e-613f-4c07-8528-ebaf89f2adf1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Función para separar el conjunto de entrenamiento y de test.\n",
    "img_train, img_test, mask_train, mask_test = train_test_split(img_array, mask_array, test_size = 0.2, random_state= 42)\n",
    "\n",
    "# Se realiza un reshape para asegurar la forma del tensor adecuada: #img x W x H x C | donde W es el ancho de las imagenes, H es el alto y C el número de canales (gray C=1, RGB C=3)\n",
    "\n",
    "img_train = np.array(img_train).reshape(len(img_train), img_side_size, img_side_size, 1)\n",
    "img_test = np.array(img_test).reshape(len(img_test), img_side_size, img_side_size, 1)\n",
    "mask_train = np.array(mask_train).reshape(len(mask_train), img_side_size, img_side_size, 1)\n",
    "mask_test = np.array(mask_test).reshape(len(mask_test), img_side_size, img_side_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantum part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "\n",
    "# dev = qml.device('qiskit.aer', wires=4, backend='aer_simulator_extended_stabilizer', ibmqx_token=IBM_TOKEN , shots=1) # OK\n",
    "# dev = qml.device('qiskit.aer', wires=4, backend='aer_simulator_matrix_product_state', ibmqx_token=IBM_TOKEN , shots=1) # OK\n",
    "dev = qml.device('default.qubit', wires=4, shots=1024) # OK\n",
    "# dev = qml.device(\"lightning.gpu\", wires=4) # OK\n",
    "\n",
    "\n",
    "# Random circuit parameters\n",
    "rand_params = np.random.uniform(high=2 * np.pi, size=(1, 4))\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(phi):\n",
    "    # Encoding of 4 classical input values\n",
    "    for j in range(4):\n",
    "        qml.RY(np.pi * phi[j], wires=j)\n",
    "    # Random quantum circuit\n",
    "    qml.RandomLayers(rand_params, wires=list(range(4)))\n",
    "    # print(\"*** Measuring...\")\n",
    "    # Measurement producing 4 classical output values\n",
    "    output = [qml.expval(qml.PauliZ(j)) for j in range(4)]\n",
    "    # print(\"*** Output:\", output)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quanv(image):\n",
    "    w = 256\n",
    "    h = 256\n",
    "    channels = 1 # B/N\n",
    "    \"\"\"Convolves the input image with many applications of the same quantum circuit.\"\"\"\n",
    "    out = np.zeros((w, h, channels))\n",
    "    q_results = [0]\n",
    "    # Loop over the coordinates of the top-left pixel of 2X2 squares\n",
    "    for j in range(0, w, 2):\n",
    "        for k in range(0, h, 2):\n",
    "            # Process a squared 2x2 region of the image with a quantum circuit\n",
    "\n",
    "                q_results = circuit(\n",
    "                    [\n",
    "                        image[j, k, 0],\n",
    "                        image[j, k + 1, 0],\n",
    "                        image[j + 1, k, 0],\n",
    "                        image[j + 1, k + 1, 0]\n",
    "                    ]\n",
    "                )\n",
    "                # Assign expectation values to different channels of the output pixel (j, k)\n",
    "                for c in range(channels):\n",
    "                    out[j, k, c] = q_results[c]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "PREPROCESS = True\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def preprocess_image(img, idx):\n",
    "    \"\"\"Preprocess a single image using quantum processing.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Processing Image: {idx + 1}\")\n",
    "        return quanv(img)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Image {idx}: Unexpected error - {e}\")\n",
    "    return None  # Return None if processing fails\n",
    "\n",
    "def preprocess_images(images):\n",
    "    \"\"\"Preprocess a batch of images using threading for efficiency.\"\"\"\n",
    "    q_images = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(preprocess_image, img, idx): idx for idx, img in enumerate(images)}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:  # Only append if processing was successful\n",
    "                q_images.append(result)\n",
    "\n",
    "    return np.asarray(q_images)\n",
    "\n",
    "def save_or_load(filename, data_func):\n",
    "    \"\"\"Save data to a file or load it if already exists.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        logging.info(f\"Loading {filename}\")\n",
    "        return np.load(filename)\n",
    "    else:\n",
    "        data = data_func()\n",
    "        np.save(filename, data)\n",
    "        logging.info(f\"Saved {filename}\")\n",
    "        return data\n",
    "\n",
    "if PREPROCESS:\n",
    "    q_train_images = save_or_load(\"q_train_images.npy\", lambda: preprocess_images(img_train))\n",
    "    q_test_images = save_or_load(\"q_test_images.npy\", lambda: preprocess_images(img_test))\n",
    "    q_train_masks = save_or_load(\"q_train_masks.npy\", lambda: preprocess_images(mask_train))\n",
    "    q_test_masks = save_or_load(\"q_test_masks.npy\", lambda: preprocess_images(mask_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Example input array (replace with your actual array)\n",
    "arr = q_train_images[2]\n",
    "\n",
    "# 1. Clip the values to the range [0, 1]\n",
    "arr = np.clip(arr, 0, 1)\n",
    "\n",
    "# 2. Scale the values to the range [0, 255]\n",
    "arr = (arr * 255).astype(np.uint8)\n",
    "\n",
    "# 3. Remove the single channel dimension if necessary (shape: H x W)\n",
    "arr = np.squeeze(arr)\n",
    "\n",
    "# 4. Convert to a PIL image\n",
    "img = Image.fromarray(arr)\n",
    "\n",
    "# 5. Show the image\n",
    "img.show()\n",
    "# Optional: Save the image as PNG\n",
    "img.save(\"training_image.png\")\n",
    "\n",
    "img = Image.open(\"training_image.png\")\n",
    "    \n",
    "## Visualizar imagen e independientemente su máscara\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax1.imshow(img, cmap = \"gray\")\n",
    "plt.title(\"image\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Example input array (replace with your actual array)\n",
    "arr = q_train_masks[0]\n",
    "\n",
    "# 1. Clip the values to the range [0, 1]\n",
    "arr = np.clip(arr, 0, 1)\n",
    "\n",
    "# 2. Scale the values to the range [0, 255]\n",
    "arr = (arr * 255).astype(np.uint8)\n",
    "\n",
    "# 3. Remove the single channel dimension if necessary (shape: H x W)\n",
    "arr = np.squeeze(arr)\n",
    "\n",
    "# 4. Convert to a PIL image\n",
    "img = Image.fromarray(arr)\n",
    "\n",
    "# 5. Show the image\n",
    "img.show()\n",
    "# Optional: Save the image as PNG\n",
    "img.save(\"training_mask_image.png\")\n",
    "img = Image.open(\"training_mask_image.png\")\n",
    "    \n",
    "## Visualizar imagen e independientemente su máscara\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax1.imshow(img, cmap = \"gray\")\n",
    "plt.title(\"mask\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cf2de2de-21a1-4a9a-ad2c-5c18452c0212",
    "_uuid": "a37bf47b-7ff7-4460-a40a-402f8ce4c1b9"
   },
   "source": [
    "## 4. Construyendo una red U-net\n",
    "\n",
    "* Las métricas utilizadas para evaluar los resultados son el coeficiente Dice e IoU (Intersection over Union):\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\">\n",
    "<img src=\"https://www.researchgate.net/publication/328671987/figure/fig4/AS:688210103529478@1541093483784/Calculation-of-the-Dice-similarity-coefficient-The-deformed-contour-of-the-liver-from.ppm\" width=\"500\" height=\"300\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"width:100%;text-align: center;\">\n",
    "<img src=\"https://i.imgur.com/yJp0n0n.png\" width=\"500\" height=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2c47c23f-bb34-43df-b3d0-56dd2a08f6b9",
    "_uuid": "fd3a063e-7045-4bfd-8961-15531db4dc15",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Función que calcula la medida Dice \n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 1) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + 1)\n",
    "\n",
    "# Función que calcula la pérdida según el Dice.\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "# Función Dice para test\n",
    "def dice_coef_test(y_true, y_pred):\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    union = tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f)\n",
    "    if union==0: return 1\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return 2. * intersection / union\n",
    "\n",
    "# Intersection over Union metric\n",
    "def IOU(y_true,y_pred):\n",
    "    intersection = tf.keras.backend.sum(y_true * y_pred)\n",
    "    sum_ = tf.keras.backend.sum(y_true) + tf.keras.backend.sum(y_pred)\n",
    "    jac = (intersection + 1) / (sum_ - intersection + 1)\n",
    "    return jac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura U-net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Enable memory growth to avoid runtime memory issues\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error setting GPU memory growth: {e}\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "401ef65d-c48a-4d84-b461-b5943d5b1bb6",
    "_uuid": "6608e42d-3369-436f-a11d-f8e52a6535a2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.activations import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "\n",
    "def unet(input_size=(img_side_size,img_side_size,1)):\n",
    "    inputs = Input(input_size)\n",
    "    \"\"\"\n",
    "    codifica la imagen en una representación abstracta de las características de la imagen aplicando \n",
    "    una secuencia de bloques convolucionales que disminuyen gradualmente la altura y la anchura de la representación\n",
    "    \"\"\"\n",
    "    # codificador (parte izquierda de la “U”)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    \"\"\"\n",
    "    decodifica la representación de la imagen en una máscara binaria aplicando una secuencia de convoluciones ascendentes.\n",
    "    que aumenta gradualmente la altura y la anchura de la representación hasta el tamaño de la imagen original y disminuye el número de \n",
    "    de canales al número de clases que estamos segmentando\n",
    "    \"\"\"\n",
    "    # decodificador (parte derecha de la “U”)\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    \n",
    "    #\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    return Model(inputs=[inputs], outputs=[conv10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def unet(input_size=(128, 128, 1)):\n",
    "    class UNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(UNet, self).__init__()\n",
    "\n",
    "            # Encoder (left side of the U)\n",
    "            self.enc1 = self.conv_block(input_size[2], 32)\n",
    "            self.enc2 = self.conv_block(32, 64)\n",
    "            self.enc3 = self.conv_block(64, 128)\n",
    "            self.enc4 = self.conv_block(128, 256)\n",
    "            self.enc5 = self.conv_block(256, 512)\n",
    "\n",
    "            # Decoder (right side of the U)\n",
    "            self.up_conv6 = self.upconv_block(512, 256)\n",
    "            self.dec6 = self.conv_block(512, 256)\n",
    "\n",
    "            self.up_conv7 = self.upconv_block(256, 128)\n",
    "            self.dec7 = self.conv_block(256, 128)\n",
    "\n",
    "            self.up_conv8 = self.upconv_block(128, 64)\n",
    "            self.dec8 = self.conv_block(128, 64)\n",
    "\n",
    "            self.up_conv9 = self.upconv_block(64, 32)\n",
    "            self.dec9 = self.conv_block(64, 32)\n",
    "\n",
    "            # Output layer\n",
    "            self.final_conv = nn.Conv2d(32, 1, kernel_size=(1, 1))\n",
    "\n",
    "        def conv_block(self, in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding='same'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding='same'),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "        def upconv_block(self, in_channels, out_channels):\n",
    "            return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=(2, 2), stride=(2, 2), padding='same')\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Encoder\n",
    "            enc1 = self.enc1(x)\n",
    "            pool1 = F.max_pool2d(enc1, kernel_size=(2, 2))\n",
    "\n",
    "            enc2 = self.enc2(pool1)\n",
    "            pool2 = F.max_pool2d(enc2, kernel_size=(2, 2))\n",
    "\n",
    "            enc3 = self.enc3(pool2)\n",
    "            pool3 = F.max_pool2d(enc3, kernel_size=(2, 2))\n",
    "\n",
    "            enc4 = self.enc4(pool3)\n",
    "            pool4 = F.max_pool2d(enc4, kernel_size=(2, 2))\n",
    "\n",
    "            enc5 = self.enc5(pool4)\n",
    "\n",
    "            # Decoder\n",
    "            up6 = torch.cat((self.up_conv6(enc5), enc4), dim=1)\n",
    "            dec6 = self.dec6(up6)\n",
    "\n",
    "            up7 = torch.cat((self.up_conv7(dec6), enc3), dim=1)\n",
    "            dec7 = self.dec7(up7)\n",
    "\n",
    "            up8 = torch.cat((self.up_conv8(dec7), enc2), dim=1)\n",
    "            dec8 = self.dec8(up8)\n",
    "\n",
    "            up9 = torch.cat((self.up_conv9(dec8), enc1), dim=1)\n",
    "            dec9 = self.dec9(up9)\n",
    "\n",
    "            # Final output\n",
    "            return torch.sigmoid(self.final_conv(dec9))\n",
    "\n",
    "    return UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = unet(input_size=(img_side_size,img_side_size,1)) # Creamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a54f2ff5-b6c7-434a-a174-1bfb2842a48e",
    "_uuid": "6591b273-83cc-4173-8228-36200b0329b3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile( # Opciones de compilación\n",
    "    optimizer=Adam(learning_rate=5*1e-4), \n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[dice_coef, 'binary_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred_f = y_pred.view(-1)\n",
    "    intersection = (y_true_f * y_pred_f).sum()\n",
    "    return (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)\n",
    "\n",
    "# Example training setup\n",
    "if __name__ == \"__main__\":\n",
    "    img_side_size = 128  # Define your image side size here\n",
    "    model = unet()  # Create an instance of your model\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5 * 1e-4)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Sample data\n",
    "    input_tensor = torch.randn(1, 1, img_side_size, img_side_size)  # Example input\n",
    "    target_tensor = torch.randint(0, 2, (1, 1, img_side_size, img_side_size)).float()  # Example target\n",
    "\n",
    "    # Training Loop\n",
    "    num_epochs = 5\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_tensor)  # Get model predictions\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, target_tensor)\n",
    "        \n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        # Calculate metrics\n",
    "        with torch.no_grad():\n",
    "            dice = dice_coef(target_tensor, torch.sigmoid(outputs))  # Calculate Dice coefficient\n",
    "            accuracy = ((torch.sigmoid(outputs) > 0.5) == target_tensor).float().mean()  # Calculate accuracy\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Dice Coefficient: {dice.item()}, Accuracy: {accuracy.item()}\")\n",
    "\n",
    "    # Save the model (optional)\n",
    "    torch.save(model.state_dict(), 'unet_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d7fe6d48-d991-4b28-8a43-b9fb78b44187",
    "_uuid": "8c298b4d-bd20-4101-894a-8ff54449ef82"
   },
   "source": [
    "## 5. Entrenamiento del modelo U-Net\n",
    "\n",
    "Para nuestro entrenamiento utilizaremos `epochs = 30`, que es el número de veces que recorreremos el conjunto de lotes de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e369763e-f767-4b47-b497-7634aab6256c",
    "_uuid": "84c6dd44-0ed0-455b-9e4a-53f95a67b9e1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"epoch_nr\": 30,\n",
    "    \"batch_size\": 1,\n",
    "}\n",
    "\n",
    "# Callback, si después de 10 epocas, la pérdida no se reduce o cambia. Se para el entrenamiento de forma temprana.\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    patience=10\n",
    ") \n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        x = q_train_images, \n",
    "        y = q_train_masks, \n",
    "        validation_data = (q_test_images, q_test_masks), \n",
    "        epochs= params[\"epoch_nr\"],\n",
    "        batch_size= params[\"batch_size\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "22631736-b9dd-4bd2-80bc-da5c53621566",
    "_uuid": "d4794b61-1bda-4950-8e81-edb222412b89"
   },
   "source": [
    "## Guardado del modelo en la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "33947b73-cb44-4298-8e2f-5da2193f128e",
    "_uuid": "5f510a10-7663-45b4-8194-1a5127c53b33",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.save('./hybrid_classical_model_seg_local_Quantum_Preprocess_30_epochs_qff_256x256.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b349af31-66d9-4e22-b604-60f5cb0f9f5a",
    "_uuid": "cb64e98e-a0f2-44f9-bbb1-f493db5cec75"
   },
   "source": [
    "### Visualización del resultado\n",
    "Aquí crearemos una función para visualizar la imagen de vista previa, la imagen de la máscara real, la superposición sobre la máscara real y la imagen original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "89be0efc-44f1-4b5e-a58f-89989aa9c7f5",
    "_uuid": "78d93f02-dfa1-4e31-9482-348b8745a9ae",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Ensure mask_test and predictions are cast to float32 for TensorFlow compatibility.\n",
    "mask_test = tf.cast(mask_test, tf.float32)\n",
    "\n",
    "Nimgs = len(q_test_images)  # Número total de imágenes\n",
    "img_array = q_test_images\n",
    "\n",
    "@interact(img_num=(0, Nimgs - 1, 1))\n",
    "def test_on_image(img_num):\n",
    "    \"\"\"Visualiza predicciones y métricas para imágenes de prueba.\"\"\"\n",
    "    \n",
    "    # Model prediction and reshaping\n",
    "    pred = model.predict(img_array[img_num].reshape(1, img_side_size, img_side_size, 1))\n",
    "    pred = tf.cast(pred, tf.float32)  # Ensure pred is float32\n",
    "    pred = tf.where(pred > 0.5, 1.0, 0.0)  # Binarize predictions\n",
    "\n",
    "    # Calculate Dice and IOU\n",
    "    dice = dice_coef_test(y_true=mask_test[img_num], y_pred=pred)\n",
    "    iou = IOU(y_true=mask_test[img_num], y_pred=pred)\n",
    "\n",
    "    # Create a figure to display results\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Display Prediction\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(pred.numpy().reshape(img_side_size, img_side_size), cmap=\"PuBu\")\n",
    "    plt.title(\"Predicción\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Display Ground Truth Mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask_test[img_num].numpy().reshape(img_side_size, img_side_size), cmap=\"viridis\")\n",
    "    plt.title(\"Máscara real\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Display Image with Contours\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(img_array[img_num].reshape(img_side_size, img_side_size), cmap=\"gray\")\n",
    "    edges_est1 = segmentation.clear_border(\n",
    "        np.squeeze(mask_test[img_num].numpy().reshape(img_side_size, img_side_size))\n",
    "    )\n",
    "    edges_est2 = segmentation.clear_border(\n",
    "        np.squeeze(pred.numpy().reshape(img_side_size, img_side_size))\n",
    "    )\n",
    "    plt.contour(edges_est1, [0.5], colors=['red'])\n",
    "    plt.contour(edges_est2, [0.5], colors=['blue'])\n",
    "    plt.title('Superposición')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display metrics and save the result\n",
    "    plt.suptitle(f'Dice: {dice:.2%} and IOU: {iou:.2%}', fontsize=28)\n",
    "    plt.savefig('prediction.png', \n",
    "                bbox_inches='tight', pad_inches=1, facecolor='white')\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d77f5e6f-c84c-477e-bc38-030bdf8ad0c9",
    "_uuid": "f200a5be-52af-4623-b4c7-f8f2da06ccf6"
   },
   "source": [
    "## 6. Métricas y validación  \n",
    "+ **Cross-entropy (pérdida)**: para cuantificar la diferencia entre las dos distribuciones de probabilidad (entrenamiento y validación).\n",
    "* **Coeficiente Dice**: El coeficiente de Dice es un estadístico utilizado para medir la similitud de dos muestras, una de las métricas más utilizadas en el contexto de la segmentación de imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4c1b1df4-94d6-42db-9371-85fbe9171918",
    "_uuid": "ea3e7423-374b-4fb4-8f35-3e640eceeb66",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_metrics(history):\n",
    "    fig = plt.figure(figsize = (15,15))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(history.history[\"loss\"], label = \"loss entrenamiento\")\n",
    "    plt.plot(history.history[\"val_loss\"], label = \"loss validación\")\n",
    "    plt.title(\"Validación x Entrenamiento: Entropía binaria\", fontsize=18, y=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Entropia binaria\")\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(history.history[\"dice_coef\"], label = \"Coeficiente Dice entrenameinto\")\n",
    "    plt.plot(history.history[\"val_dice_coef\"], label = \"Coeficiente Dice validación\")\n",
    "    plt.title(\"Validación x Entrenamiento: Coeficiente Dice\", fontsize=18, y=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Coeficiente Dice\")\n",
    "    plt.savefig('metrics.png', bbox_inches='tight', pad_inches=1, facecolor='white')\n",
    "\n",
    "    \n",
    "get_metrics(history = history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cf59e858-c975-4673-a372-1764a5a05398",
    "_uuid": "5e6a4716-baf3-4bd8-9c44-98377fa48095"
   },
   "source": [
    "### Comprobación de la validación de la prueba mediante el coeficiente Dice  \n",
    "Utilizaremos los datos de la prueba para hacer nuestra predicción y realizaremos el solapamiento entre la máscara predicha y la real. Se examinarán 141 imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0346b3a5-32ce-4740-8ab4-49d16a6fac3b",
    "_uuid": "37e3059b-2f00-449c-b6e4-082743cfdc90",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Ensure masks are float32 for compatibility\n",
    "q_test_masks = tf.cast(q_test_masks, tf.float32)\n",
    "\n",
    "dice_coefs = list()\n",
    "\n",
    "# Predict all images at once\n",
    "pred = model.predict(q_test_images)\n",
    "\n",
    "# Binarize predictions\n",
    "pred = tf.cast(pred >= 0.5, tf.float32)\n",
    "\n",
    "# Iterate over predictions and calculate Dice coefficients\n",
    "for i in tqdm(range(len(q_test_images))):\n",
    "    prediction = pred[i]\n",
    "    mask = q_test_masks[i]\n",
    "\n",
    "    # Ensure prediction and mask are float32\n",
    "    prediction = tf.cast(prediction, tf.float32)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "    # Calculate Dice coefficient\n",
    "    dice = dice_coef_test(y_true=mask, y_pred=prediction)\n",
    "    dice_coefs.append(dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5f7e7d9-c7be-4e18-b783-21d64748550a",
    "_uuid": "72bd49ab-9d25-4fe9-a3a4-8e3441ad0e7a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize =(10, 7))\n",
    "plt.hist(np.array(dice_coefs), bins = 50)\n",
    "plt.title(\"Distribución del Coeficiente Dice (141 imágenes)\",fontsize=20, y=1)\n",
    "plt.xlabel(\"Coeficiente Dice\")\n",
    "plt.ylabel(\"Número de imágenes\")\n",
    "print(f'Mediana Coef. Dice: {np.median(np.array(dice_coefs))}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "46ef42d3-f054-4a62-9581-93e8ce42561e",
    "_uuid": "b1a72537-6e2f-4a34-92a5-7fb8739d9db1",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize =(10, 7))\n",
    "plt.boxplot(np.array(dice_coefs))\n",
    "plt.title(\"Distribución del Coeficiente Dice (141 imágenes)\",fontsize=20, y=1)\n",
    "plt.ylabel(\"Coeficiente Dice\")\n",
    "print(f'Mediana Coef. Dice: {np.median(np.array(dice_coefs))}')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "685a88fc-63c6-4907-b4ea-890698abcac8",
    "_uuid": "f78e3724-c247-4708-a4dc-053b08dc0443"
   },
   "source": [
    "# Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3d5c0601-e11b-4e2f-877d-f695d288a89d",
    "_uuid": "a816d61f-d2f6-4c34-9ad7-dff92ec93049",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # librería para monitorear las funciones y sus tiempos de ejecución.\n",
    "import cv2 # Librería Open-CV para python.\n",
    "from skimage import segmentation\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "734db402-4c3b-4e38-843d-fc35e716cbd8",
    "_uuid": "49bc840d-6aac-454e-abb3-3b51a7467603",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_trained_model():\n",
    "    try:\n",
    "        K.set_floatx('float64')\n",
    "        custom_objects = {'dice_coef': {}}\n",
    "        model_path = './hybrid_classical_model_seg_local_Quantum_Preprocess_30_epochs_qff_256x256.h5'\n",
    "        model = load_model(model_path, custom_objects=custom_objects)\n",
    "        print('Model loaded OK')\n",
    "        return model\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": \"No se puede cargar el modelo: \" + str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bb0f1686-e734-471d-a99e-4e9946917d83",
    "_uuid": "1975786e-6efe-4bf2-bb8f-b5fd99a9f5de",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "83f96033-8e6f-4d27-9d16-65445b0cd1b3",
    "_uuid": "636de8b2-5e79-41e9-afae-b4f23317eaa5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Definir la función de predicción\n",
    "def predict_image():\n",
    "    # Cargar el modelo entrenado\n",
    "    model = load_trained_model()\n",
    "    if isinstance(model, dict):\n",
    "        return model\n",
    "    \n",
    "    # Procesar la imagen\n",
    "    img_side_size = 256\n",
    "    img_path = './xr-lung-segmentation.jpeg'\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return {\"error\": f\"No se pudo cargar la imagen: {img_path}\"}\n",
    "    img = cv2.resize(img, (img_side_size, img_side_size))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0  # Normalizar la imagen\n",
    "    \n",
    "    # Verificar que la imagen tenga el tamaño correcto\n",
    "    input_shape = model.input_shape[1:3]\n",
    "    if img.shape[:2] != input_shape:\n",
    "        return {\"error\": f\"El tamaño de la imagen no coincide con el tamaño de entrada del modelo: {img.shape} vs {input_shape}\"}\n",
    "    \n",
    "    # Realizar la predicción\n",
    "    pred = model.predict(img_array.reshape(1,img_side_size,img_side_size,1))\n",
    "    pred[pred>0.5] = 1.0\n",
    "    pred[pred<0.5] = 0.0\n",
    "    \n",
    "    # Crear la figura de matplotlib\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(pred.reshape(img_side_size, img_side_size), cmap = \"viridis\")\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.axis(\"off\")   \n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(img.reshape(img_side_size, img_side_size), cmap='gray')\n",
    "    edges_est = segmentation.clear_border(np.squeeze(pred.reshape(img_side_size, img_side_size)))\n",
    "    plt.contour(edges_est,[0.5],colors=['blue'])\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.savefig('./results.png', bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # Devolver los resultados\n",
    "    return {\"plot\": fig}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "da7d2a52-937e-4674-a059-62f59444d4e4",
    "_uuid": "30beb355-cc27-42e0-a561-f3239286c01e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    results = []\n",
    "\n",
    "    # Manejar errores si no se puede procesar la imagen\n",
    "    try:\n",
    "        result = predict_image()\n",
    "    except Exception as e:\n",
    "        return {\"error\": \"No se puede procesar la imagen: \" + str(e)}\n",
    "\n",
    "    results.append(result)\n",
    "    \n",
    "    return {\"results\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77226381-21da-4459-bfcc-391f1f9371b7",
    "_uuid": "7f8d0644-fb52-4151-988a-79561b6cf5fc",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-py-3-10)",
   "language": "python",
   "name": "tf-py-3-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
